{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Tidy Data </h1>\n",
    "\n",
    "<p>",
    "    The principle importance of Tidy data is vast and here we will focus on some important aspects of Tidy data. Below aare two representations of the same data. Which one looks insightful?\n",
    "</p>",
    "<h4> TABLE 1 </h4><br>\n",
    "\n",
    
    "| Name | Age   | Gender  | Experience A | Experience B |\n",
    "|---|---|---|----|---|\n",
    "| Dan  |  24   |  Male   |       2      |      3       |\n",
    "| Mike |   -   |  Male   |       -      |      4       |\n",
    "| Gina |  20   |  Female |       1      |      5       |\n",
    "\n",
    "<br><h4>TABLE 2</h4><br>\n",
    "\n",
    "\n",
    "|    NULL      |   0     |    1    |    2    |\n",
    "|---|---|----|---|\n",
    "|    Name      |  Dan    |  Mike   |  Gina   |\n",
    "|    Age       |   24    |   -     |   20    |\n",
    "|   Gender     |  Male   |  Male   |  Female |\n",
    "| Experience A |   2     |   -     |   1     |\n",
    "| Experience B |   3     |   4     |   5     |\n",
    "\n",
    "<p>\n",
    "The pavlonian answer is that the first one is. And, it can be meaningful to get insightful results. So, the main attributes of Tidy data are as follows:\n",
    "    \n",
    "- Columns represent seperate variables\n",
    "- Rows represent induvidual observations\n",
    "- Observational units form Tables\n",
    "</p>\n",
    "\n",
    "So, converting the table 1 to a tidy form we get the following:\n",
    "<br><h4> TIDY TABLE 1 </h4><br>\n",
    "\n",
    "| Name | Age   | Gender  | Experience | Value |\n",
    "|------|-------|---------|------------|-------|\n",
    "| Dan  |  24   |  Male   |     A      |   2   |\n",
    "| Dan  |  24   |  Male   |     B      |   3   |\n",
    "| Mike |   -   |  Male   |     A      |   -   |\n",
    "| Mike |   -   |  Male   |     B      |   4   |\n",
    "| Gina |  20   |  Female |     A      |   1   |\n",
    "| Gina |  20   |  Female |     B      |   5   |\n",
    "\n",
    "Tidy data is <em> better for Reporting </em>. Fixes common data problems\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Tidy data\n",
    "### Use pd.melt()\n",
    "\n",
    "<p>Melting data is the process of turning columns of your data into rows of data. Consider the DataFrames from the previous exercise. In the tidy DataFrame, the variables <code>Ozone</code>, <code>Solar.R</code>, <code>Wind</code>, and <code>Temp</code> each had their own column.\n",
    "If, however, you wanted these variables to be in rows instead, you could melt the DataFrame. In doing so, however, you would make the data untidy! This is important to keep in mind: Depending on how your data is represented, you will have to reshape it\n",
    "differently (e.g., this could make it easier to plot values).</p>\n",
    "<p>In this exercise, you will practice melting a DataFrame using <code>pd.melt()</code>. There are two parameters you should be aware of: <code>id_vars</code> and <code>value_vars</code>.\n",
    "The <code>id_vars</code> represent the columns of the data you <strong>do not</strong> want to melt (i.e., keep it in its current shape), while the <code>value_vars</code> represent the columns you <strong>do</strong> wish to melt into rows.  By default, if no <code>value_vars</code> are provided, all columns not set in the <code>id_vars</code> will be melted. This could save a bit of typing, depending on the number of columns that need to be melted.</p>\n",
    "\n",
    "### Using .pivot_table()\n",
    "\n",
    "<p>Pivoting data is the opposite of melting it. Remember the tidy form that the <code>airquality</code> DataFrame was in before you melted it? You&apos;ll now begin pivoting it back into that form using the <code>.pivot_table()</code> method!</p>\n",
    "<p>While melting takes a set of columns and turns it into a single column, pivoting will create a new column for each unique value in a specified column.</p>\n",
    "<p><code>.pivot_table()</code> has an <code>index</code> parameter which you can use to specify the columns that you <em>don&apos;t</em> want pivoted: It is similar to the <code>id_vars</code> parameter of <code>pd.melt()</code>. Two other parameters that you have to specify are <code>columns</code> (the name of the column you want to pivot), and <code>values</code> (the values to be used when the column is pivoted). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading the tb data\n",
    "tb = pd.read_csv('tb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  country  year variable  value gender age_group\n",
      "0      AD  2000     m014    0.0      m       014\n",
      "1      AE  2000     m014    2.0      m       014\n",
      "2      AF  2000     m014   52.0      m       014\n",
      "3      AG  2000     m014    0.0      m       014\n",
      "4      AL  2000     m014    2.0      m       014\n"
     ]
    }
   ],
   "source": [
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(frame=tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>We have tidied the <code>'m014'</code> column, which represents males aged 0-14 years of age. In order to parse this value, you need to extract the first letter into a new column for gender, and the rest into a column for age_group. Here, since you can parse values by position, you can take advantage of pandas' vectorized string slicing by using the str attribute of columns of type object.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing the Ebola Dataset\n",
    "\n",
    "Another common way multiple variables are stored in columns is with a delimiter. Consider the dataset containing the [Ebola cases and death counts by state and country](https://data.humdata.org/dataset/ebola-cases-2014).<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Day', 'Cases_Guinea', 'Cases_Liberia', 'Cases_SierraLeone',\n",
       "       'Cases_Nigeria', 'Cases_Senegal', 'Cases_UnitedStates', 'Cases_Spain',\n",
       "       'Cases_Mali', 'Deaths_Guinea', 'Deaths_Liberia', 'Deaths_SierraLeone',\n",
       "       'Deaths_Nigeria', 'Deaths_Senegal', 'Deaths_UnitedStates',\n",
       "       'Deaths_Spain', 'Deaths_Mali'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset\n",
    "ebola = pd.read_csv('ebola.csv')\n",
    "\n",
    "# Printing the columns of Ebola\n",
    "ebola.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation\n",
    "\n",
    "We see that <code>Cases_Guinea</code> and <code>Deaths_Guinea</code>. Here, the underscore _ serves as a delimiter between the first part (cases or deaths), and the second part (country). We can use the <code>.split()</code> method to split the string in columns. For example, using <code>Cases.Guinea.split('_')</code> will return a list <code>['Cases','Guinea']</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date  Day  type_country  counts        str_split   type country\n",
      "0    1/5/2015  289  Cases_Guinea  2776.0  [Cases, Guinea]  Cases  Guinea\n",
      "1    1/4/2015  288  Cases_Guinea  2775.0  [Cases, Guinea]  Cases  Guinea\n",
      "2    1/3/2015  287  Cases_Guinea  2769.0  [Cases, Guinea]  Cases  Guinea\n",
      "3    1/2/2015  286  Cases_Guinea     NaN  [Cases, Guinea]  Cases  Guinea\n",
      "4  12/31/2014  284  Cases_Guinea  2730.0  [Cases, Guinea]  Cases  Guinea\n"
     ]
    }
   ],
   "source": [
    "# Splitting a column with .split() and .get()\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_')\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Data\n",
    "<p>\n",
    "Data may not always come in 1 file. For example, a 5 million row datase may be broken into 5 seperate datasets which is easier to store and share. This is maily because new data may be added each data. This brings the necessity of combining the data.</p>\n",
    "\n",
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(297, 4)\n",
      "          Date/Time      Lat      Lon    Base\n",
      "0  4/1/2014 0:11:00  40.7690 -73.9549  B02512\n",
      "1  4/1/2014 0:17:00  40.7267 -74.0345  B02512\n",
      "2  4/1/2014 0:21:00  40.7316 -73.9873  B02512\n",
      "3  4/1/2014 0:28:00  40.7588 -73.9776  B02512\n",
      "4  4/1/2014 0:33:00  40.7594 -73.9722  B02512\n"
     ]
    }
   ],
   "source": [
    "# Reading the uber datasets for 3 different periods\n",
    "uber1 = pd.read_csv('uber1.csv')\n",
    "uber2 = pd.read_csv('uber2.csv')\n",
    "uber3 = pd.read_csv('uber3.csv')\n",
    "\n",
    "# Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1,uber2,uber3])\n",
    "\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining 1k files?\n",
    "What to do when there are many files: Use <code>glob()</code> function.\n",
    "\n",
    "#### Globbing\n",
    "This involves pattern matching file names using wildcards like *\n",
    "So any csv file can be read as <code>*.csv</code> and any single character file as <code>_?.csv</code>. This returns a <b> list of file names</b>. We can then use this list to load into seperate dataframes.\n",
    "\n",
    "__PLAN:__\n",
    "- Load files from globbing into pandaas\n",
    "- Add the Datafranes into a list\n",
    "- Concatenate multiple datasets at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dob_job_application_filings_subset.csv', 'ebola.csv', 'tb.csv', 'uber1.csv', 'uber2.csv', 'uber3.csv', 'winequality-red.csv', 'winequality-white.csv']\n",
      "  country  year  m014  m1524  m2534  m3544  m4554  m5564   m65  mu  f014  \\\n",
      "0      AD  2000   0.0    0.0    1.0    0.0    0.0    0.0   0.0 NaN   NaN   \n",
      "1      AE  2000   2.0    4.0    4.0    6.0    5.0   12.0  10.0 NaN   3.0   \n",
      "2      AF  2000  52.0  228.0  183.0  149.0  129.0   94.0  80.0 NaN  93.0   \n",
      "3      AG  2000   0.0    0.0    0.0    0.0    0.0    0.0   1.0 NaN   1.0   \n",
      "4      AL  2000   2.0   19.0   21.0   14.0   24.0   19.0  16.0 NaN   3.0   \n",
      "\n",
      "   f1524  f2534  f3544  f4554  f5564   f65  fu  \n",
      "0    NaN    NaN    NaN    NaN    NaN   NaN NaN  \n",
      "1   16.0    1.0    3.0    0.0    0.0   4.0 NaN  \n",
      "2  414.0  565.0  339.0  205.0   99.0  36.0 NaN  \n",
      "3    1.0    1.0    0.0    0.0    0.0   0.0 NaN  \n",
      "4   11.0   10.0    8.0    8.0    5.0  11.0 NaN  \n"
     ]
    }
   ],
   "source": [
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv3\n",
    "csv3 = pd.read_csv(csv_files[2])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv3.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19963, 123)\n",
      "  Adult Estab Applicant License # Applicant Professional Title  \\\n",
      "0         NaN             0058375                           PE   \n",
      "1         NaN             0025259                           RA   \n",
      "2         NaN             0084178                           PE   \n",
      "3         NaN             0078226                           PE   \n",
      "4         NaN             0086781                           PE   \n",
      "\n",
      "  Applicant's First Name           Applicant's Last Name    Approved Assigned  \\\n",
      "0        DOUGLAS          MASS                                   NaN      NaN   \n",
      "1        STEVEN           SAVINO                                 NaN      NaN   \n",
      "2        ASHRAF           ALI                             04/25/2013      NaN   \n",
      "3        J. BUTCH         MACUTAY JR.                            NaN      NaN   \n",
      "4        JUNHUI           JIA                                    NaN      NaN   \n",
      "\n",
      "  Base      Bin #   Block  ...  fu m014 m1524  m2534  m3544  m4554  m5564  \\\n",
      "0  NaN  1016890.0   857.0  ... NaN  NaN   NaN    NaN    NaN    NaN    NaN   \n",
      "1  NaN  5161350.0   342.0  ... NaN  NaN   NaN    NaN    NaN    NaN    NaN   \n",
      "2  NaN  1053831.0  1729.0  ... NaN  NaN   NaN    NaN    NaN    NaN    NaN   \n",
      "3  NaN  1015610.0   826.0  ... NaN  NaN   NaN    NaN    NaN    NaN    NaN   \n",
      "4  NaN  1015754.0   831.0  ... NaN  NaN   NaN    NaN    NaN    NaN    NaN   \n",
      "\n",
      "   m65  mu  year  \n",
      "0  NaN NaN   NaN  \n",
      "1  NaN NaN   NaN  \n",
      "2  NaN NaN   NaN  \n",
      "3  NaN NaN   NaN  \n",
      "4  NaN NaN   NaN  \n",
      "\n",
      "[5 rows x 123 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3044: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "C:\\Users\\Public\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:15: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "# Implementing the concat function\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)\n",
    "    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
